{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "## import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mecab = Mecab()\n",
    "#fm.findSystemFonts()\n",
    "plt.rcParams['font.family']= [\"NanumGothicCoding\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False\n",
    "# GPU 환경 설정하기\n",
    "# assert tf.test.is_gpu_available() == True, 'GPU 설정을 확인하세요.'\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 가져옵니다.\n",
    "spam = pd.read_csv('./AI_MODEL_2022-10-12_09-48-48/spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Web발신]골든브릿지에서손실을보셨나요??http://bitly.kr/bRGtq[FW]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Web발신][대//박]가(.원)입코(WN)드MEP.com</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Web발신](광고)이정미님아직도주식하시나요http://pf.kakao.com/_u...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Web발신]송승용님사람이모이는곳에는이유가있습니다하루~정보공개is.gd/JsJP</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0        [Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA  spam\n",
       "1   [Web발신]골든브릿지에서손실을보셨나요??http://bitly.kr/bRGtq[FW]  spam\n",
       "2                   [Web발신][대//박]가(.원)입코(WN)드MEP.com  spam\n",
       "3  [Web발신](광고)이정미님아직도주식하시나요http://pf.kakao.com/_u...  spam\n",
       "4       [Web발신]송승용님사람이모이는곳에는이유가있습니다하루~정보공개is.gd/JsJP  spam"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 데이터를 수치형으로 변환합니다.\n",
    "spam['label'] = spam['label'].map({'spam':1, 'ham':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>nouns</th>\n",
       "      <th>morphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 박춘규, 회원, 손절, 주식, 월급, 배, 법]</td>\n",
       "      <td>[[, Web, 발신, ], 박춘규, 회원, 님, 손절, 주식, 은, 그만, 월급,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Web발신]골든브릿지에서손실을보셨나요??http://bitly.kr/bRGtq[FW]</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 골든, 브릿지, 손실]</td>\n",
       "      <td>[[, Web, 발신, ], 골든, 브릿지, 에서, 손실, 을, 보, 셨, 나요, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Web발신][대//박]가(.원)입코(WN)드MEP.com</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 대, 박, 원, 코]</td>\n",
       "      <td>[[, Web, 발신, ], [, 대, /, /, 박, ], 가, (, ., 원, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Web발신](광고)이정미님아직도주식하시나요http://pf.kakao.com/_u...</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 광고, 이정미, 주식]</td>\n",
       "      <td>[[, Web, 발신, ], (, 광고, ), 이정미, 님, 아직, 도, 주식, 하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Web발신]송승용님사람이모이는곳에는이유가있습니다하루~정보공개is.gd/JsJP</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 송승용, 사람, 곳, 이유, 하루, 정보, 공개]</td>\n",
       "      <td>[[, Web, 발신, ], 송승용, 님, 사람, 이, 모이, 는, 곳, 에, 는,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20095</th>\n",
       "      <td>[Web발신]이상투자황기봉님\"이재훈\"투자컨설담당자배정완료.담당자확인▼http://b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 상투, 자황, 기봉, 이재훈, 투자, 컨, 설, 담당자, 배정, 완료, 담...</td>\n",
       "      <td>[[, Web, 발신, ], 이, 상투, 자황, 기봉, 님, \", 이재훈, \", 투...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20096</th>\n",
       "      <td>[Web발신](광고)조마?峙???충?矛?%:까지　↓주소Joma.카JM톡무료거부</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 광고, 조마, 충, 矛, 주소, 카, 톡, 무료, 거부]</td>\n",
       "      <td>[[, Web, 발신, ], (, 광고, ), 조마, ?, 峙, ?, ??, 충, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20097</th>\n",
       "      <td>[Web발신]김환택님지금이라도늦지않았습니다.월긴급정보http://pf.kakao.c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 김, 환택, 지금, 월, 긴급, 정보]</td>\n",
       "      <td>[[, Web, 발신, ], 김, 환택, 님, 지금, 이라도, 늦, 지, 않, 았,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20098</th>\n",
       "      <td>[Web발신]년노하우명품구매대행논스토리nonstory.com항상아낌없는사랑과성원에감...</td>\n",
       "      <td>0</td>\n",
       "      <td>[발신, 년, 노하우, 명품, 구매, 대행, 스토리, 사랑, 성원, 감사]</td>\n",
       "      <td>[[, Web, 발신, ], 년, 노하우, 명품, 구매, 대행, 논, 스토리, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20099</th>\n",
       "      <td>[Web발신]최현님월일특급정보!https://pf.kakao.com/_SfcsT</td>\n",
       "      <td>1</td>\n",
       "      <td>[발신, 최현, 월일, 특급, 정보]</td>\n",
       "      <td>[[, Web, 발신, ], 최현, 님, 월일, 특급, 정보, !, https, :...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20089 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0            [Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA      1   \n",
       "1       [Web발신]골든브릿지에서손실을보셨나요??http://bitly.kr/bRGtq[FW]      1   \n",
       "2                       [Web발신][대//박]가(.원)입코(WN)드MEP.com      1   \n",
       "3      [Web발신](광고)이정미님아직도주식하시나요http://pf.kakao.com/_u...      1   \n",
       "4           [Web발신]송승용님사람이모이는곳에는이유가있습니다하루~정보공개is.gd/JsJP      1   \n",
       "...                                                  ...    ...   \n",
       "20095  [Web발신]이상투자황기봉님\"이재훈\"투자컨설담당자배정완료.담당자확인▼http://b...      1   \n",
       "20096        [Web발신](광고)조마?峙???충?矛?%:까지　↓주소Joma.카JM톡무료거부      1   \n",
       "20097  [Web발신]김환택님지금이라도늦지않았습니다.월긴급정보http://pf.kakao.c...      1   \n",
       "20098  [Web발신]년노하우명품구매대행논스토리nonstory.com항상아낌없는사랑과성원에감...      0   \n",
       "20099       [Web발신]최현님월일특급정보!https://pf.kakao.com/_SfcsT      1   \n",
       "\n",
       "                                                   nouns  \\\n",
       "0                        [발신, 박춘규, 회원, 손절, 주식, 월급, 배, 법]   \n",
       "1                                      [발신, 골든, 브릿지, 손실]   \n",
       "2                                       [발신, 대, 박, 원, 코]   \n",
       "3                                      [발신, 광고, 이정미, 주식]   \n",
       "4                       [발신, 송승용, 사람, 곳, 이유, 하루, 정보, 공개]   \n",
       "...                                                  ...   \n",
       "20095  [발신, 상투, 자황, 기봉, 이재훈, 투자, 컨, 설, 담당자, 배정, 완료, 담...   \n",
       "20096               [발신, 광고, 조마, 충, 矛, 주소, 카, 톡, 무료, 거부]   \n",
       "20097                         [발신, 김, 환택, 지금, 월, 긴급, 정보]   \n",
       "20098          [발신, 년, 노하우, 명품, 구매, 대행, 스토리, 사랑, 성원, 감사]   \n",
       "20099                               [발신, 최현, 월일, 특급, 정보]   \n",
       "\n",
       "                                                  morphs  \n",
       "0      [[, Web, 발신, ], 박춘규, 회원, 님, 손절, 주식, 은, 그만, 월급,...  \n",
       "1      [[, Web, 발신, ], 골든, 브릿지, 에서, 손실, 을, 보, 셨, 나요, ...  \n",
       "2      [[, Web, 발신, ], [, 대, /, /, 박, ], 가, (, ., 원, ...  \n",
       "3      [[, Web, 발신, ], (, 광고, ), 이정미, 님, 아직, 도, 주식, 하...  \n",
       "4      [[, Web, 발신, ], 송승용, 님, 사람, 이, 모이, 는, 곳, 에, 는,...  \n",
       "...                                                  ...  \n",
       "20095  [[, Web, 발신, ], 이, 상투, 자황, 기봉, 님, \", 이재훈, \", 투...  \n",
       "20096  [[, Web, 발신, ], (, 광고, ), 조마, ?, 峙, ?, ??, 충, ...  \n",
       "20097  [[, Web, 발신, ], 김, 환택, 님, 지금, 이라도, 늦, 지, 않, 았,...  \n",
       "20098  [[, Web, 발신, ], 년, 노하우, 명품, 구매, 대행, 논, 스토리, no...  \n",
       "20099  [[, Web, 발신, ], 최현, 님, 월일, 특급, 정보, !, https, :...  \n",
       "\n",
       "[20089 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "spam['nouns'] = [mecab.nouns(i) for i in spam['text']]\n",
    "spam['morphs'] = [mecab.morphs(i) for i in spam['text']]\n",
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_sum(nouns):\n",
    "    return ' '.join(nouns)\n",
    "spam['text'] = spam['nouns'].apply(nouns_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16071,), (4018,), (16071,), (4018,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train validation set으로 분리합니다.\n",
    "x = spam['text']\n",
    "y = spam['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2022)\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:1804: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    \n",
    "    return x_train, x_val\n",
    "\n",
    "x_train_ngram, x_val_ngram = ngram_vectorize(x_train, y_train, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 30000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "    print(max_length)\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index\n",
    "\n",
    "x_train_seq, x_val_seq, word_idx = sequence_vectorize(x_train, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams 방식으로 vectorize한 데이터의 shape을 확인해봅니다.\n",
    "# 모델 학습시에 활용 가능하도록 전처리 데이터를 저장해보도록 하겠습니다.\n",
    "import pickle\n",
    "x_train_ngram\n",
    "\n",
    "\n",
    "with open('./x_train_ngram', 'wb') as f:\n",
    "    pickle.dump(x_train_ngram, f)\n",
    "with open('./x_val_ngram', 'wb') as f:\n",
    "    pickle.dump(x_val_ngram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 형태로 vectorize한 데이터의 shape을 확인해봅니다.\n",
    "# 모델 학습시에 활용 가능하도록 전처리 데이터를 저장해보도록 하겠습니다.\n",
    "with open('./x_train_seq', 'wb') as f:\n",
    "    pickle.dump(x_train_seq, f)\n",
    "with open('./x_val_seq', 'wb') as f:\n",
    "    pickle.dump(x_val_seq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 데이터의 shape을 확인하고 저장합니다.\n",
    "with open('./y_train', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open('./y_val', 'wb') as f:\n",
    "    pickle.dump(y_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
